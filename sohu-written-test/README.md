###搜狐笔试题 

#####项目设计-思考过程-问题总结
---

###目录 
1. 项目说明 
2. 项目设计 
3. 项目总结 
--- 
###1. 项目说明 
请设计一个系统，自动完成对于手机搜狐(http://m.sohu.com/)系统可靠性的检测。具体要求： 

* 定时递归检测所有m.sohu.com域名的页面以及这些页面上的链接的可达性，即有没有出现不可访问情况。 
* m.sohu.com域名页面很多，从各个方面考虑性能优化。 
* 对于错误的链接记录到日志中，日志包括：连接，时间，错误状态等。 
* 考虑多线程的方式实现 
--- 
###2.项目设计

开发环境说明：

1. Python3.5
2. MongoDB数据库

第三方库:

1. pymongo
2. BitVector
3. BeautifulSoup

刚看到这道笔试题的时候，我主要在想这几个问题(难点)：

1. 怎么判断一个url是否不可访问？
2. 采用深度优先还是广度优先？
3. url怎么去重？
4. 多线程或者多进程中数据共享怎么怎么解决？

#### 2.1 难点实现思路

##### 2.1.1 判断url是否可达

1. requests中有个status_code，由于搜狐页面基本都是get方法，爬取中也不会涉及http的其他协议，所以status\_code如果不是200.说明页面是不可达的。
2. 观察发现，即使页面不存在，status_code还是200，说明搜狐进行了重定向，所以解析那个页面，如果有404信息，则这个url也属于不可达。

##### 2.1.2 采用深度优先还是广度优先？

1. 深度优先，思路比较简单，就是递归。如果url很多的时候，递归其实会占用内存的，所以并不是一个很好的方案，而且不好做多线程。
2, 广度优先，思路就是将抓取的url放在队列的尾端，每次从头部取url，多线程也比较好做。

结论：采取广度优先算法

##### 2.1.3 url去重
整个手机搜狐网站来说，如果把一个url看做一个点，那么最终构成的一定是图结构，而不是树结构。而爬虫的抓取一定要是树结构，不然会在图里面进行死循环，所以url的去重就很重要，去重的方案：

1. **放在列表： **如果直接把url放在一个列表里面进行遍历排查，对于千万级别的url数量，内存肯定不够用，而且遍历也耗时间，所以pass掉。

2. **放在数据库： **将url放在数据库去重，这是可行的，查询直接用MongoDB中的```find({'url':url})```进行查询是否已经存在，缺点是查询还是会消耗一定的时间。

3. **布隆过滤器： **《数学之美》这本书中提到的布隆过滤器算法，将url作为集合中的一个元素，将这个元素利用随机映射函数映射到二进制向量中，这种算法优点是内存消耗很少，查询时间也快。缺点是有一定误判率，而且只能判断元素是否在集合中，而不是对集合中的元素进行操作。(其实可以，但是操作会对其他url的二进制位的值有影响)

总结：方案1和2都是可取的做法。

##### 2.1.4 多线程&多进程
1. **多线程： **Python中的多线程有个GIL，而且在这个项目中，待抓取的url列表是一个共享的数据，当一个线程对其进行操作的时候，其他线程必须等待，多线程可以作为一个方案。

2. **多线程： **Python中的有个```multiprocessing.Pool```类可以作为进程池，但是共享数据无法同步，问题就是出现在待抓取url这个列表，所以如果把待抓取url放在数据库中，是可以实现的。

总结：如果用了多线程，那么就不用数据库了。如果用了多进程，那就不用布隆过滤器，直接用数据库。

---
#### 2.2 项目架构图

根据以上，我设计出两个方案：

##### 2.2.1 方案一
![方案1](https://github.com/AJKipper/python/blob/master/sohu-written-test/pics/case1.png)

##### 2.2.2 方案二
![方案2](https://github.com/AJKipper/python/blob/master/sohu-written-test/pics/case2.png)


### 3. 项目总结

这个项目中我思考最多的地方：

1. 怎么写出最优的url去重方案
2. Python多线程或者多进程在这个项目中怎么更好地使用

学到的东西：

1. 多线程和多进程的使用
2. 布隆过滤器算法的使用
3. 巩固Python的基础





